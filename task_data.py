tasks = [
     {
        "course": "Python基礎【3013】",
        "answer": "# 関数Ai_demyの中を作成してください\ndef Ai_demy(numbers):\n    # ここに処理を記述してください\n    for n in numbers:\n        if n % 5 == 0 and n % 7 == 0:\n            print(\"{}:Aidemy\".format(n))\n        elif n % 5 == 0:\n            print(\"{}:Ai\".format(n))\n        elif n % 7 == 0:\n            print(\"{}:demy\".format(n))\n        else:\n            print(n)\n    \n# データの定義\nnumbers = list(range(1,36))\n# 関数の実行\nAi_demy(numbers)",
        "correct_response": "Python基礎【3013】の添削課題のご提出ありがとうございます。\n添削結果を返却いたします。\nーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n\nPython基礎【3013】の添削課題を返却いたします。\n正解です。\n\nご提出いただきましたコードを拝見しますと本講座の内容について\nしっかりと、ご理解いただいているように感じます。\n素晴らしいです。\n\n引き続きぜひこの調子で頑張ってください。\n次回の提出もお待ちしております。",
        "incorrect_response": "Python基礎【3013】 の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\nーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n\nPython基礎【3013】 の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    
    {
        "course": "Python基礎【3013】_応用問題",
        "answer": "# バイナリーサーチを行う関数\ndef binary_search(numbers, target_number):\n    # 最小値を仮決め\n    low = 0\n    # 範囲内の最大値\n    high = len(numbers) - 1\n    # 目的地を探し出すまでループ\n    while low <= high:\n        # 中央値を求める（index）\n        middle = (low + high) // 2\n        # 中央値のnumbersの値とtarget_numberが等しい場合\n        if numbers[middle] == target_number:\n            # 出力\n            middle+=1\n            print(\"{1}は{0}番目にあります\".format(middle, target_number))\n            # 終了させる\n            break\n        # 中央値のnumbersの値がtarget_numberより小さい場合\n        elif numbers[middle] < target_number:\n            low = middle + 1\n        # 中央値のnumbersの値がtarget_numberより大きい場合\n        else:\n            high = middle - 1\n\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\ntarget_number = 11\nbinary_search(numbers, target_number)",
        "correct_response": "{{コース}}の添削課題のご提出ありがとうございます。\n添削結果を返却いたします。\nーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n\nPython基礎【3013】の応用添削課題を返却いたします。\n正解です。\n\n大変素晴らしいです。\nバイナリーサーチは、ソートされたリストからターゲット値を効率的に検索するアルゴリズムです。\n\n解答例を添付いたしますので、お手隙のときにご確認ください。\n\n次回の提出もお待ちしております。",
        "incorrect_response": "{{コース}}の添削課題のご提出ありがとうございます。\n添削結果を返却いたします。\nーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n\nPython基礎【3013】の応用添削課題を返却いたします。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先にお進みください。\n次回の提出もお待ちしております。"
    },

    {
        "course": "【新】ライブラリ「NumPy」基礎（数値計算）【4004】",
        "answer": "# 必要なライブラリをimport\nimport numpy as np\nimport time\nfrom numpy.random import rand\n\n# 行、列の大きさ\nN = 5000\n\n# 配列の初期化\nmat = rand(N, N)\n\n# Numpyの機能を使わずに計算\n\n# 開始時間の取得\nstart = time.time()\n\n# for文を使って、1番目の軸に沿って平均を計算\nmean_not_numpy = []\nfor i in range(N):\n    mean_not_numpy.append(sum(mat[i]) / len(mat[i]))\n\n# 出力形式を整えるため、numpy配列に変換\nprint(np.array(mean_not_numpy))\nprint(f'Total time when not using NumPy：{(time.time() - start):.2f}[sec]')\nprint()\n\n# NumPyを使って計算\n\n# 開始時間の取得\nstart = time.time()\n\n# NumPyの機能を使って、1番目の軸に沿って平均を計算\nmean_numpy = mat.mean(axis=1)\n\nprint(mean_numpy)\nprint(f'Total time when using NumPy：{(time.time() - start):.2f}[sec]')",
        "correct_response": "{{コース}}の添削課題のご提出ありがとうございます。\n添削結果を返却いたします。\nーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n\n「NumPy」基礎（数値計算）【4004】の添削課題のご提出お疲れさまでした。\n正解です。\n\n今回の添削課題において、\nfor文とNumpyによる計算処理速度を比較いただいたことで、\nNumpyの計算処理性能の高さを実感いただけたと思います。\n\n引き続き、頑張っていきましょう。\n次回の課題提出をお待ちしております。",
        "incorrect_response": "{{コース}}の添削課題のご提出ありがとうございます。\n添削結果を返却いたします。\nーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n\n「NumPy」基礎（数値計算）【4004】の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    }
    ,
    {
        "course": "【新】ライブラリ「NumPy」基礎（数値計算）【4004】_応用問題",
        "answer": "#====応用課題=====\nimport numpy as np\n\n# 乱数の初期化\nnp.random.seed(0)\n\n# 指定された大きさの画像を乱数を用いて生成する関数\n# 仮引数mは画像の縦の大きさ、nは画像の横の大きさです\ndef make_image(m, n):\n    \n    # m×n行列の各成分を0~5の値でランダムに満たしてください\n    image = np.random.randint(0, 6, (m, n))\n        \n    return image\n\n\n# 渡された行列の一部を変更する関数\ndef change_matrix(matrix):\n    # 与えられた行列の形を取得し、shapeに代入してください\n    shape = matrix.shape\n    \n    # 行列の各成分について、変更するかしないかをランダムに決めた上で\n    # 変更する場合は0~5のいずれかの整数にランダムに入れ替えてください\n    for i in range(shape[0]):\n        for j in range(shape[1]):\n            if np.random.randint(0, 2)==1:\n                matrix[i][j] = np.random.randint(0, 6, 1)\n    return matrix\n\n# ランダムに画像を作成\nimage1 = make_image(3, 3)\nprint(image1)\nprint()\n\n# ランダムに変更を適用する\nimage2 = change_matrix(np.copy(image1))\nprint(image2)\nprint()\n\n# image1とimage2の差分を計算し、image3に代入してください\nimage3 = image2 - image1\nprint(image3)\nprint()\n\n# image3の各成分が絶対値である行列をもとめimage3に再代入してください\nimage3 = np.abs(image3)\n\n# image3を出力\nprint(image3)\n",
        "correct_response": "「NumPy」基礎（数値計算）【4004】の応用添削課題のご提出お疲れさまでした。\n正解です。\n\n適切にコードが書かれていて大変素晴らしいです\n解答例も添付いたしますので、お手すきの際にご確認いただけると幸いです。\n\n引き続き、頑張っていきましょう。\n次回の課題提出をお待ちしております。",
        "incorrect_response": "「NumPy」基礎（数値計算）【4004】の応用添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
        "course": "〔新〕ライブラリ「Pandas」基礎（表計算）【4014】",
        "answer": "import pandas as pd\n\nstore_data = {\n    \"store\": [\"shibuya\", \"shinjuku\", \"yokohama\", \"meguro\", \"ikebukuro\"],\n    \"ID\": [1, 2, 3, 4, 5]\n}\nstore_df = pd.DataFrame(store_data)  # store_dfを作成\n\ndata = {\"ID\": [1, 2, 3, 3, 2, 1],\n        \"product\": [\"banana\", \"orange\", \"orange\", \"grape\", \"banana\", \"peach\"],\n        \"price\": [200, 1000, 800, 100, 250, 900],\n        \"quantity\": [1, 2, 1, 2, 3, 2]}\ndf = pd.DataFrame(data)  # dfを作成\n\nprint(df)  # DataFrame 、dfの出力\nprint()\nprint(store_df)  # DataFrame、store_dfの出力\nprint()\n\n# 問題1\n# dfのインデックスが０から４までの要素、カラム名を出力してください。\ndf_1 = df.head()\nprint(df_1)\nprint()\n\n# 問題2\n# df とstore_dfをkeyをIDとして完全外部結合してください。\ndf_2 = pd.merge(df, store_df, on='ID', how='outer')\nprint(df_2)\nprint()\n\n# 問題3\n# df とstore_dfをkeyをIDとして内部結合してください。\ndf_3 = pd.merge(df, store_df, on='ID', how='inner')\nprint(df_3)\nprint()\n\n# 問題4\n# 問題3の回答にて作成したdf_3とgroupbyメソッドを用いてstore毎のID、price、quantityの平均値を出力してください。\ndf_4 = df_3.loc[:,[\"price\", \"quantity\", \"store\"]].groupby('store').mean()\nprint(df_4)\nprint()\n\n# 問題5\n# 問題3の回答にて作成したdf_3とdescribeメソッドを用いてID、price、quantityの要約統計量を出力してください。\ndf_5 = df_3.describe()\nprint(df_5)",
        "correct_response": "{{コース}}の添削課題のご提出ありがとうございます。\n添削結果を返却いたします。\nーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n\n{{コース}}の添削課題の提出お疲れ様でした。\n正解です。\n\nしっかりとデータの表示やデータフレーム同士の\n内部・外部結合、および要約統計量を出力できております。\n\n解答例をお送りいたしますので、お手隙の際にご確認のほど宜しくお願いいたします。\n\nPandasは実際のデータ分析でも多用される代表的なライブラリです。\nこの機会にしっかり覚えていただければと思います\n\nこの調子でがんばっていきましょう！\n次回の添削問題もお待ちしております。",
        "incorrect_response": "{{コース}}の添削課題のご提出ありがとうございます。\n添削結果を返却いたします。\nーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n\n{{コース}}の添削課題の提出お疲れ様でした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": "【新】ライブラリ「Matplotlib」基礎（可視化）【4044】_応用問題",
    "answer": "#=====応用課題=====\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport time\n\nnp.random.seed(100)\nX = 0  # 的に当たった回数です\n\n# 試行回数Nを指定してください。\nN = 1000\n# 四分円の境界の方程式[y=√1-x^2 (0<=x<=1)]を描画しています。\ncircle_x = np.arange(0, 1, 0.001)\ncircle_y = np.sqrt(1 - circle_x * circle_x)\nplt.figure(figsize=(5, 5))\nplt.plot(circle_x, circle_y)\n\n# N回の試行にかかる時間を計測します。\nstart_time = time.perf_counter()\n\n#プロット用の空の配列を用意\ninternal_x = []\ninternal_y = []\nexternal_x = []\nexternal_y = []\n\n# N回の試行を行っています。\nfor i in range(N):\n    # 0から1の間で一様乱数を発生させ、変数score_xに格納してください。\n    score_x = np.random.rand()\n    # 0から1の間で一様乱数を発生させ、変数score_yに格納してください。\n    score_y = np.random.rand()\n    if score_x * score_x + score_y * score_y <= 1:\n        # 的に入ったものはinternal_x, internal_yに追加してください。\n        internal_x.append(score_x)\n        internal_y.append(score_y)\n        \n        # 得点Xを1追加してください\n        X = X + 1\n    else:\n        # 的から外れたものはexternal_x, external_yに追加してください。\n        external_x.append(score_x)\n        external_y.append(score_y)\n\n# piの近似値をここで計算してください。\npi = 4*float(X)/float(N)\n\n# モンテカルロ法の実行時間を計算しています。\nend_time = time.perf_counter()\ntime = end_time - start_time\n\n# 円周率の結果を表示。\nprint(\"円周率:%.6f\" % pi)\nprint(\"実行時間:%f\" % (time))\n\n# 散布図を描画してください。四分円内にある点は赤で、四分円外にある点は青で描画してください。\nplt.scatter(internal_x, internal_y, color=\"r\")\nplt.scatter(external_x, external_y, color=\"b\")\n\n# 結果を表示\nplt.grid(True)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()",
    "correct_response": "【新】ライブラリ「Matplotlib」基礎（可視化）【4044】 の応用添削問題のご提出ありがとうございます。\n添削結果を返却致します。\nーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n\n「Matplotlib」基礎（可視化）【4044】 の応用添削課題お疲れ様でした。\n正解です。\n\n今回の応用添削課題では、モンテカルロ法についての問題でした。\nシッカリと理解されていることがうかがえます。\n\n今後もこの調子で頑張ってください。\nそれでは次回の添削問題もお待ちしております。",
    "incorrect_response": "【新】「Matplotlib」基礎（可視化）【4044】の応用添削課題のご提出ありがとうございます。\n添削結果を返却いたします。\nーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n\n「Matplotlib」基礎（可視化）【4044】の応用添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": "データクレンジング【4050】",
    "answer": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import datasets\nfrom sklearn.metrics import accuracy_score\n\n#データのロード\nwine_data = datasets.load_wine()\ndata = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)  # 説明変数のDataFrmeを設定\ntarget = pd.DataFrame(wine_data.target, columns=['target'])  # 目的変数のDataFrmeを設定\n\n# 説明変数のDataFraemと目的変数のDataFrameをつなげて　wine_df を作成\nwine_df = pd.concat([data,target],axis=1)\nwine_df = wine_df.drop(columns=['magnesium','proline']) # 値のスケールが異なる'magnesium','proline'の列を削除します\nprint(wine_df.head())\n\n# 欠損値のあるデータの作成\n# 以下のデータを欠損\n# 10. color_intensity 色の強さ\n# 11. hue 色合い\n# 12. od280/od315_of_diluted_wines ワインの希釈度合い\nnp.random.seed(32)\n\nmask5 = np.random.rand(len(wine_df))<0.05 # 5% のデータを欠損させる\nmask3 = np.random.rand(len(wine_df))<0.03 # 3% のデータを欠損させる\nwine_df.loc[mask5,['color_intensity']] = np.nan # color_intensityのデータを5%欠損させます\nwine_df.loc[mask3,['hue']] = np.nan # hueのデータを3%欠損させます\nwine_df.loc[mask5,['od280/od315_of_diluted_wines']] = np.nan # od280/od315_of_diluted_winesのデータを5%欠損させます\n\n# 欠損値の状況を確認します。\nprint(wine_df.isnull().sum())\n\n# 欠損値を埋めてください\n# dropnaを用いてリストワイズ削除を行なってくだい\nwine_df_listwise = wine_df.dropna()\n\n# 欠損値を埋めてください\n# fillnaを用いてNaNの部分に０を代入してください\nwine_df_zero = wine_df.fillna(0)\n\n# 欠損値を埋めてください\n# fillnaを用いてNaNの部分に、NaNの入っている前(上)の行の値を代入してください\nwine_df_ffill = wine_df.fillna(method='ffill')\n\n# 欠損値を埋めてください\n# fillnaを用いてNaNの部分にその列の平均値を代入してください\nwine_df_mean = wine_df.fillna(wine_df.mean())\n\n# 欠損値の処理を行なったデータを使ってRandomForestClassifierで学習を行います\nwine_df_all = [wine_df_listwise,wine_df_zero,wine_df_ffill,wine_df_mean]\n\nfor i,wine_df_tmp in enumerate(wine_df_all):\n    y = wine_df_tmp[\"target\"]\n    X = wine_df_tmp.drop(\"target\", axis=1)\n\n    # データ分割\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    # 学習\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n      \n    if i == 0 : df_type = 'listwise'\n    if i == 1 : df_type = 'zero'\n    if i == 2 : df_type = 'ffill'\n    if i == 3 : df_type = 'mean'\n    \n    print(\"{:12s}を用いた欠損値補完:  accuracy_score = {:<.3f}\".format(df_type,accuracy_score(y_test, y_pred)))",
    "correct_response": "データクレンジング【4050】 の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n添削問題お疲れ様でした。\n正解です。\nコードの確認をさせて頂いたところ、しっかりと実装できていました。\n本講座の内容をしっかりとご理解いただいているように思います。\n\n今回学んだデータクレンジングの手法は機械学習においてほとんどの場合で行います。\n解答例もお送りいたしますので、ご確認いただけますと幸いです。\n\n今後もこの調子で頑張ってください。\n次回の添削問題もお待ちしております。",
    "incorrect_response": "データクレンジング【4050】の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": "スクレイピング入門【4070】",
    "answer": "# 必要なライブラリのインポート\nimport requests\nfrom bs4 import BeautifulSoup\n\n# はじめに、ベースとなる1ページ目のURLを定義します\nbase_url = \"https://scraping-web-site.aidemy.net\"\n\n# スクレイピングするURLを引数に取る関数を定義します\n# 日付とタイトルのデータをまとめたリストを返します\ndef scraping(url):\n    \n    # urlのデータを読み込み、BeautifulSoupによるパースをしてください\n    response =  requests.get(url)\n    soup = BeautifulSoup(response.text, \"lxml\")\n    \n    # 手順1に従い、データの入ったボックスを取得して変数boxesに代入してください\n    boxes = soup.find_all(\"li\")\n    \n    # 手順3の、ページ内全写真分のデータを収めるリストの定義します\n    results = []\n    \n    # 手順2に従い、1つずつのデータボックスから必要なデータを取得してください\n    for box in boxes:\n        # データボックスから直接日付データを取得し、変数dateに代入してください\n        date = box.find(\"p\", class_=\"release-date\")\n        # dateのtext要素を取り出して、変数dateに再代入してください\n        date = date.text\n        \n        # 以下2行は日付書式変更部分です\n        date = date.split(\"-\")\n        date = \"{}-{:0>2}-{:0>2}\".format(date[0], date[1], date[2])\n        \n        \n        # データボックスから直接タイトルデータを取得し、変数titleに代入してください\n        title = box.find(\"h3\")\n        # titleのtext要素を取り出して、変数titleに再代入してください\n        title = title.text\n        \n        # 手順2に従い、日付とタイトルのリストを作成してください\n        data = [date, title]\n        \n        # 手順3に従い、リストresultsに追加してください\n        results.append(data)\n    \n    return results\n\n# ベースのURLを読み込み、BeautifulSoupでパースしてください\nresponse = requests.get(base_url)\nsoup = BeautifulSoup(response.text, \"lxml\")\n\n# ページの下の1~5のボタンのタグ要素から全5ページのURLを取得してください\nurls = soup.find_all(\"a\")\n\nurl_list = []\n# url_listにそれぞれのページのURLを追加してください\nfor url in urls:\n    url = base_url + url.get(\"href\")\n    url_list.append(url)\n\ndata = []\n# url_listの中の各URLに対して、定義したscraping関数を実行していきます\nfor url in url_list:\n    # scraping関数の戻り値である1つにまとめたリストを、変数dataに足し集めてください\n    data += scraping(url)\n    \n# 全写真分のデータリストを日付基準で並び替えてください\ndata.sort()\n\n# 並び替えたdataを「日付: タイトル」の形で出力してください\nfor date, title in data:\n    print(\"{}: {}\".format(date, title))",
    "correct_response": "スクレイピング入門【4070】 の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n'''\nスクレイピング入門【4070】　の添削課題にお取り組みいただきありがとうございました。\n正解です。\n\nご提出いただきましたコードを拝見いたしますと、\n本講座の内容をしっかりとご理解いただいております。\n素晴らしいです。\n\n今回は下記の理解度を問うような課題となっておりました。\n-  urlからhtmlを取ってきてBeautifulSoupによってパースできるか\n- 取りたい情報のHTMLタグやクラス情報が理解できているか\n- データボックスからいろいろな過程を経て欲しい情報(日付とタイトル)を確実に取得できているか\n\n引き続きこの調子で頑張ってください。\n次回のご提出もお待ちしております。\n\"\"\"",
    "incorrect_response": "スクレイピング入門【4070】 の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n'''\nスクレイピング入門【4070】　の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。\n\"\"\""
    },
    {
    "course": "データハンドリング【4080】",
    "answer": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# 株価データを読み込んでください\nstock_data =  pd.read_csv('./5130_rnn_lstm_data/uniqlo_training_stocks_2012-2017.csv')\n\n#　stock_dataの値を出力 Dateが日付順になっていないことを確認\nprint(stock_data.head(10))\n\n# Dateをstr型からdatetime型に変換\nstock_data['Date'] = pd.to_datetime(stock_data['Date'])\n# Dateを日付順にソート\nsorted_stock_data = stock_data.sort_values(['Date'])\n\n# Stock Trading列を抽出してください\nstock_trading = sorted_stock_data['Stock Trading'] # 解答\n\n# plot()メソッドでプロットしてください\nstock_trading.plot() # 解答\n\n# グラフを出力してください\nplt.show() # 解答\n \n# sorted_stock_dataを'sorted_stock_data'という名前でExcelファイルへ書き出してください\nsorted_stock_data.to_excel('sorted_stock_data.xlsx') # 解答\nprint(sorted_stock_data.head(10))",
    "correct_response": "データハンドリング【4080】の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n添削課題お疲れさまでした。\n正解です。\nご提出いただきましたコードを拝見いたしますと、本講座の内容を\nバッチリご理解いただいていることが伺えます。\n\n今回、データハンドリングで身につけたテキストデータに対する操作や、\nデータの扱い方は今後機械学習を行っていく上で非常に重要な技術です。\n\n次回の添削問題もお待ちしております。",
    "incorrect_response": "データハンドリング【4080】の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\nデータハンドリング【4080】の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": [
        "教師あり学習（回帰）【5010】",
        "〔2024年度シラバス対応〕教師あり学習（回帰） ※E資格対策講座【6901】"
    ],
    "answer": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n# 必要なモジュールを追記してください。\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\n\n# データの読み込み\nconcrete_data = pd.read_excel(\"./5010_regression_data/Concrete_Data.xls\")\n\nconcrete_train_X, concrete_test_X, concrete_train_y, concrete_test_y = train_test_split(\n    concrete_data.drop('Concrete compressive strength(MPa, megapascals) ', axis=1),\n    concrete_data['Concrete compressive strength(MPa, megapascals) '], random_state=42)\n\n# 以下にコードを記述してください。\nmax_score = 0\nbest_model = \"\"\n# 線形回帰\nmodel = LinearRegression()\nmodel.fit(concrete_train_X, concrete_train_y)\nscore = model.score(concrete_test_X, concrete_test_y)\nif max_score < score:\n    max_score = score\n    best_model = \"線形回帰\"\n# ラッソ回帰\nmodel = Lasso()\nmodel.fit(concrete_train_X, concrete_train_y)\nscore = model.score(concrete_test_X, concrete_test_y)\nif max_score < score:\n    max_score = score\n    best_model = \"ラッソ回帰\"\n# リッジ回帰\nmodel = Ridge()\nmodel.fit(concrete_train_X, concrete_train_y)\nscore = model.score(concrete_test_X, concrete_test_y)\nif max_score < score:\n    max_score = score\n    best_model = \"リッジ回帰\"\n    \nprint(\"モデル:{}\".format(best_model))\nprint(\"決定係数:{}\".format(max_score))",
    "correct_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削問題へ取り組みいただき、ありがとうございます．\n正解です。\n\nモデルを作成し、精度を出力することが問題なくできており、大変素晴らしいです！\n解答例も添付いたしますので、ご確認いただければ幸いです。\n\nこの調子で学習を進めていただければと思います。\nそれでは次回の添削問題もお待ちしております。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} のの添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": [
        "教師あり学習（分類）【5020】",
        "〔2024年度シラバス対応〕教師あり学習（分類） ※E資格対策講座【6902】"
    ],
    "answer": "import scipy.stats\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n#手書き数字の画像データの読み込み\ndigits_data = load_digits()\n\n#読み込んだ digits_data の内容確認\nprint(dir(digits_data))\n\n#読み込んだ digits_data の画像データの確認\nimport matplotlib.pyplot as plt\nfig , axes = plt.subplots(2,5,figsize=(10,5),\n                         subplot_kw={'xticks':(),'yticks':()})\nfor ax,img in zip(axes.ravel(),digits_data.images):\n    ax.imshow(img)\nplt.show()\n\n# グリッドサーチ：ハイパーパラメーターの値の候補を設定\nmodel_param_set_grid = {SVC(): {\"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"] ,\n                                \"C\": [0.001,0.01,0.1,1,10,100] ,\n                                \"decision_function_shape\": [\"ovr\", \"ovo\"],\n                                \"random_state\": [42]}}\n\n# ランダムサーチ：ハイパーパラメーターの値の候補を設定\nmodel_param_set_random =  {SVC(): {\"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"] ,\n                                   \"C\": scipy.stats.uniform(0.00001, 1000) ,\n                                   \"decision_function_shape\": [\"ovr\", \"ovo\"],\n                                   \"random_state\": [42]}}\n\n#トレーニングデータ、テストデータの分離\ntrain_X, test_X, train_y, test_y = train_test_split(digits_data.data, digits_data.target, random_state=0)\n\n#条件設定\nmax_score = 0\n\n#グリッドサーチ\nfor model, param in model_param_set_grid.items():\n    clf = GridSearchCV(model, param)\n    clf.fit(train_X, train_y)\n    pred_y = clf.predict(test_X)\n    score = f1_score(test_y, pred_y, average=\"micro\")\n\n    if max_score < score:\n        max_score = score\n        best_param = clf.best_params_\n        best_model = model.__class__.__name__\n\nprint(\"グリッドサーチ\")\nprint(\"ベストスコア:{}\".format(max_score))\nprint(\"モデル:{}\".format(best_model))\nprint(\"パラメーター:{}\".format(best_param))\nprint()\n\n#条件設定\nmax_score = 0\n\n#ランダムサーチ\nfor model, param in model_param_set_random.items():\n    clf =RandomizedSearchCV(model, param)\n    clf.fit(train_X, train_y)\n    pred_y = clf.predict(test_X)\n    score = f1_score(test_y, pred_y, average=\"micro\")\n\n    if max_score < score:\n        max_score = score\n        best_param = clf.best_params_\n        best_model = model.__class__.__name__\n\nprint(\"ランダムサーチ\")\nprint(\"ベストスコア:{}\".format(max_score))\nprint(\"モデル:{}\".format(best_model))\nprint(\"パラメーター:{}\".format(best_param))",
    "correct_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削問題へ取り組みいただき、ありがとうございます．\n正解です。\n\n今回は下記を問うような課題でした。\n-グリッドサーチおよびランダムサーチを用いて最適なモデルとそのハイパーパラメータを求められるか\n\nコードの確認をさせて頂いたところ、問題なく実装できていました。\n本講座の内容をしっかりとご理解いただいているように思います。\n素晴らしいです。\n\n解答例も併せてお送りしますので、お手隙の際にご確認いただけますと幸いです。\n\nぜひこの調子で頑張ってください。\n次回のご提出もお待ちしております。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} のの添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n今回は下記を問うような課題でした。\n-グリッドサーチおよびランダムサーチを用いて最適なモデルとそのハイパーパラメータを求められるか\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": "教師なし学習【5030】",
    "answer": "# 必要なライブラリのインポート\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# PCAクラスを定義します\nclass PCA:\n    def __init__(self):\n           pass\n    \n    # 以下のfitメソッドを完成させてください\n    def fit(self, X):\n        # 引数X, yをインスタンス変数X, yに代入します\n        self.X = X\n        # 受け取ったデータXを標準化してください\n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n        # 標準化したデータの相関行列を計算してください\n        R = np.corrcoef(X.T)\n        # 相関行列を固有値分解し、固有値と固有ベクトルを求めてください\n        eigvals, eigvecs = np.linalg.eigh(R)\n        # 2次元に圧縮する特徴変換行列を作成してください\n        W = np.c_[eigvecs[:, -1], eigvecs[:, -2]]\n        self.vec = eigvecs\n        # データXを特徴変換して得たデータをインスタンス変数dataに代入してください\n        self.data = X.dot(W)\n\n# 実験データを読み込み、DataFrameを作成します\ndf = pd.read_csv(\"./5030_unsupervised_learning_data/Data_Cortex_Nuclear.csv\")\n\n# 今回は使用しないタンパク質のデータである、21列目～80列目を削除してください\ndf = df.drop(df.columns[range(21, 81)], axis=1)\n\n# 今回使用するクラスのマウスは計29匹なので、29×15=435個のデータを用います\n# 435行目以降は使わないので、434行目までを抽出してください\ndf = df.iloc[:435]\n\n# 欠損値nanを含む行をリストワイズ削除してください\ndroped_df = df.dropna()\n\n# 最終列のclassを抽出し、ラベルyとして定義してください\ny = droped_df[\"class\"]\n\n# 0列目のMouseIDと目的変数であるclassを削除し、特徴量Xとして定義してくだい\nX = droped_df.drop([\"MouseID\", \"class\"], axis=1)\n\n# PCAクラスを用いてデータを分析してください\nclf = PCA()\nclf.fit(X)\n\n# 圧縮したデータを取得し、matplotlibで表示してください\nX_pca = clf.data\ncolors = [\"r\", \"g\", \"b\"]\n\nfor label, color in zip(y.unique(), colors):\n    # 横軸(第一引数)にはX_pcaの0列目を、縦軸(第二引数)にはX_pcaの1列目を表示します\n    plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=color, label=label)\nplt.legend()\nplt.show()",
    "correct_response": "教師なし学習【5030】の添削問題のご提出ありがとうございます。\n添削結果を返却致します\n------\n\n添削課題お疲れ様でした。\n正解です。\nコードの確認をさせて頂いたところ、問題なく実装できていました。\n本講座の内容をしっかりとご理解いただいているように思います。\n素晴らしいです。\n\n今回の添削課題のポイントは以下の2点でした。\n- PCAの手法の流れを理解しており、PCAクラスを実装できる\n- DataFrameの行削除, 列削除, na除去が実装できる\n\nこの添削問題は、他の添削問題に比べ記述部分も多く、かなり難易度として高かったかと思いますが、\nしっかりと理解されていると感じられます。\n\n解答例も添付いたしますので、お手すきの際にご確認いただければと思います。\n\n今後も引き続き頑張っていきましょう！！\n次回の添削問題もお待ちしております。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n今回の添削課題のポイントは以下の2点でした。\n- PCAの手法の流れを理解しており、PCAクラスを実装できる\n- DataFrameの行削除, 列削除, na除去が実装できる\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": [
        "自然言語処理基礎【5050】",
        "〔2024年度シラバス対応〕自然言語処理基礎 ※E資格対策講座【6903】"
    ],
    "answer": "import glob\nimport random\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom janome.tokenizer import Tokenizer\n\n\ndef load_livedoor_news_corpus():\n    category = {\n        'dokujo-tsushin': 1,\n        'it-life-hack': 2,\n        'kaden-channel': 3,\n        'livedoor-homme': 4,\n        'movie-enter': 5,\n        'peachy': 6,\n        'smax': 7,\n        'sports-watch': 8,\n        'topic-news': 9\n    }\n    docs = []\n    labels = []\n\n    for c_name, c_id in category.items():\n        files = glob.glob(\"./5050_nlp_data/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n\n        text = ''\n        for file in files[:30]: # 実行時間の関係上読み込むデータを制限しています。\n            with open(file, 'r', errors='ignore') as f:\n                lines = f.read().splitlines()\n\n                # 1,2行目に書いたあるURLと時間は関係ないので取り除きます。\n                url = lines[0]\n                datetime = lines[1]\n                subject = lines[2]\n                body = \"\".join(lines[3:])\n                text = subject + body\n\n            docs.append(text)\n            labels.append(c_id)\n\n    return docs, labels\n\n\ndocs, labels = load_livedoor_news_corpus()\n\n# indices は0からドキュメントの数までの整数をランダムに並べ替えた配列\nrandom.seed()\nindices = list(range(len(docs)))\n# 9割をトレーニングデータとする\nseparate_num = int(len(docs) * 0.9)\n\nrandom.shuffle(indices)\n\ntrain_data = [docs[i] for i in indices[0:separate_num]]\ntrain_labels = [labels[i] for i in indices[0:separate_num]]\ntest_data = [docs[i] for i in indices[separate_num:]]\ntest_labels = [labels[i] for i in indices[separate_num:]]\n\n# テキストを分割する関数\nt=Tokenizer()\ndef tokenize1(text):\n    tokens = t.tokenize(text)\n    noun = []\n    for token in tokens:\n        noun.append(token.surface)            \n    return noun\n\n# Tf-idfを用いてtrain_dataをベクトル化し、引数tokenizerにtokenize1を指定してください。\nvectorizer = TfidfVectorizer(tokenizer=tokenize1)\ntrain_matrix = vectorizer.fit_transform(train_data)\n\n# ランダムフォレストを用いて分類をおこなってください\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(train_matrix, train_labels)\n\n# テストデータを変換\ntest_matrix = vectorizer.transform(test_data)\n\n# 分類結果を表示\nprint(clf.score(train_matrix, train_labels))\nprint(clf.score(test_matrix, test_labels))\n\n# 単語の抽出\ndef tokenize2(text):\n    tokens = t.tokenize(text)\n    noun = []\n    for token in tokens:\n        # 「名詞」「動詞」「形容詞」「形容動詞」を取り出してください\n        partOfSpeech = token.part_of_speech.split(',')[0]\n\n        if partOfSpeech == '名詞':\n            noun.append(token.surface)\n        if partOfSpeech == '動詞':\n            noun.append(token.surface)\n        if partOfSpeech == '形容詞':\n            noun.append(token.surface)\n        if partOfSpeech == '形容動詞':\n            noun.append(token.surface)\n          \n    return noun\n\n# 単語の抽出して学習\nt = Tokenizer()\nvectorizer = TfidfVectorizer(tokenizer=tokenize2)\ntrain_matrix = vectorizer.fit_transform(train_data)\ntest_matrix = vectorizer.transform(test_data)\nclf.fit(train_matrix, train_labels)\n\n# 結果を表示\nprint(clf.score(train_matrix, train_labels))\nprint(clf.score(test_matrix, test_labels))",
    "correct_response": "自然言語処理基礎【5050】 の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n添削問題お疲れ様ですコード確認させていただきました。\n正解です。素晴らしいです。\n\n今回の課題は下記を問うような課題となっておりました。\n- Tfidfを用いてベクトル化できているか\n- ベクトル化したデータで分類モデルを作成できているか\n- `tokenize`関数内で特定品詞の抽出が実装できているか\n\nご提出いただきましたコードを確認させていただきますと、本講座の内容を\nバッチリとご理解いただいていることが伺えます。\n\n解答例も添付いたしますので、お手すきの際にご確認ください。\n\nぜひこの調子で頑張ってください。\n次回のご提出もお待ちしております。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n今回の課題は下記を問うような課題となっておりました。\n- Tfidfを用いてベクトル化できているか\n- ベクトル化したデータで分類モデルを作成できているか\n- `tokenize`関数内で特定品詞の抽出が実装できているか\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出は不要ですので、次の講座にお進みください。\n\n次回のご提出もお待ちしております。"
    },
    {
    "course": "時系列解析Ⅰ（統計学的モデル）【5060】",
    "answer": "# 実行には時間がかかる場合があります\n\nimport warnings\nimport itertools\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# 1.データの読み込み\nsales_car = pd.read_csv(\n    filepath_or_buffer=\"./5060_tsa_data/monthly-car-sales-in-quebec-1960.csv\",\n    dtype={'x02': 'float64'})\n\n# 2.データの整理\n# indexに期間を代入してください。期間は\"1960-01-31\"から\"1968-12-31\"としてください\nindex = pd.date_range('1960-01-31', '1968-12-31', freq='M')\n\n# sales_carのインデックスにindexを代入してください\nsales_car.index = index\n\n# sales_carの\"Month\"カラムを削除してください\ndel sales_car['Month']\n\n# orderの最適化関数\ndef selectparameter(DATA, s):\n    p = d = q = range(0, 2)\n    pdq = list(itertools.product(p, d, q))\n    seasonal_pdq = [(x[0], x[1], x[2], s) for x in list(itertools.product(p, d, q))]\n    parameters = []\n    BICs = np.array([])\n    for param in pdq:\n        for param_seasonal in seasonal_pdq:\n            try:\n                mod = sm.tsa.statespace.SARIMAX(DATA,\n                                                order=param,\n                                                seasonal_order=param_seasonal)\n                results = mod.fit()\n                parameters.append([param, param_seasonal, results.bic])\n                BICs = np.append(BICs, results.bic)\n            except:\n                continue\n    return parameters[np.argmin(BICs)]\n\n# 5.モデルの構築\n\n# SARIMAモデルを用いて時系列解析をしてください\n# 周期は月ごとのデータであることも考慮してs=12となります\n# orderはselectparameter関数の0インデックス, seasonal_orderは1インデックスに格納されています\nbest_params = selectparameter(sales_car, 12)\nSARIMA_sales_car = sm.tsa.statespace.SARIMAX(sales_car.sales, order=best_params[0],\n                                             seasonal_order=best_params[1],\n                                             enforce_stationarity=False, enforce_invertibility=False).fit()\n\n# 予測\n\n# predに予測期間での予測値を代入してください\npred = SARIMA_sales_car.predict('1968-01-31', '1972-01-31')\n\n# グラフを可視化してください。予測値は赤色でプロットしてください\nplt.plot(sales_car)\nplt.plot(pred, \"r\")\nplt.show()",
    "correct_response": "時系列解析Ⅰ（統計学的モデル）【5060】 の添削問題のご提出ありがとうございます。\n添削結果を返却致します\n---\n\n時系列解析Ⅰ（統計学的モデル）【5060】の添削課題を返却いたします。\n正解です。\n\nご提出いただいたコードを確認いたしますと、sarimaモデルのパラメータの決定方法や\n学習までバッチリとご理解されていることが伺えます。\n\n解答例をお送り致しますので、お手すきの際にご確認いただけますと幸いです。\n\n引き続きこの調子で頑張ってください。\n次回のご提出もお待ちしております。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": "ディープラーニング基礎【5090】",
    "answer": "import numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Dropout, Input, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import fetch_california_housing\nimport matplotlib.pyplot as plt\n\n# 出力結果の固定\ntf.random.set_seed(0)\n\n%matplotlib inline\n\n# sample_dataからデータセットを読み込みます\ntrain = pd.read_csv('sample_data/california_housing_train.csv')  # sample_dataからデータを読み込みます\n\nY = train.median_house_value # yにはMedHouseValが格納されています\nX = train.drop(columns='median_house_value') # xにはMedHouseVal以外の列が格納されています\n\n# 説明変数のデータから緯度・経度（latitude・longitude）のデータを削除します\nX=X.drop(columns=['latitude','longitude'])\n\n# テストデータとトレーニングデータに分割します\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n\nmodel = Sequential()\nmodel.add(Dense(32, input_dim = 6))\nmodel.add(Activation('relu'))\n#上にならって、ユニット数128の中間層をmodel.addで追加してください。\nmodel.add(Dense(128))\n#上にならって、活性化関数reluをmodel.addで追加してください。\nmodel.add(Activation('relu'))\nmodel.add(Dense(1))  \n\n# 損失関数にmse、最適化関数にadamを採用\nmodel.compile(loss='mse', optimizer='adam')\n\n\n# モデルを学習させます\nhistory = model.fit(X_train, y_train, \n                    epochs= 30,   # エポック数 \n                    batch_size= 16,  # バッチサイズ\n                    verbose=1, \n                    validation_data=(X_test, y_test) )\n\n# 予測値を出力します\ny_pred = model.predict(X_test)\n\n# 二乗誤差を出力します\nmse= mean_squared_error(y_test, y_pred)\nprint(\"REG RMSE : %.2f\" % (mse** 0.5))\n\n# epoch毎の予測値の正解データとの誤差を表しています\n# バリデーションデータのみ誤差が大きい場合、過学習を起こしています\n\ntrain_loss=history.history['loss']\nval_loss=history.history['val_loss']\nepochs=len(train_loss)\n\nplt.plot(range(epochs), train_loss, marker = '.', label = 'train_loss')\nplt.plot(range(epochs), val_loss, marker = '.', label = 'val_loss')\nplt.legend(loc = 'best')\nplt.grid()\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()",
    "correct_response": "ディープラーニング基礎【5090】 の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n---\n\nディープラーニング基礎の添削課題、お疲れ様でした。\n正解です。\nコードの確認をさせて頂いたところ、問題なく実装できていました。\n本講座の内容をしっかりとご理解いただいているように感じます。\n\n解答例もお送りいたしますので、お手隙の際にご確認ください。\nぜひこの調子で頑張ってください。\n\n次回の添削課題もお待ちしております。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": "CNNを用いた画像認識【5100】",
    "answer": "from tensorflow.keras import optimizers\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Input\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# データのロードをしてください\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n#input_tensorの定義をして、vggのImageNetによる学習済みモデルを作成してください\ninput_tensor = Input(shape=(32, 32, 3))\nvgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n\n# 特徴量抽出部分のモデルを作成しています\ntop_model = Sequential()\ntop_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\ntop_model.add(Dense(256, activation='sigmoid'))\ntop_model.add(Dropout(0.5))\ntop_model.add(Dense(10, activation='softmax'))\n\n# vgg16とtop_modelを連結してください\nmodel = Model(inputs=vgg16.input, outputs=top_model(vgg16.output))\n\n# 以下のfor文を完成させて、15層目までの重みを固定させてください\nfor layer in model.layers[:15]:\n    layer.trainable = False\n\n#　学習の前に、モデル構造を確認してください\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.SGD(learning_rate=1e-4, momentum=0.9),\n              metrics=['accuracy'])\n\n# すでに学習済みのモデルを保存している場合、以下のように学習済みモデルを取得できます\n# model.load_weights('param_vgg_15.weights.h5')\n\n# バッチサイズ32、エポック数は3で学習を行ってください\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=3)\n\n# 以下の式でモデルを保存できます\nmodel.save_weights('param_vgg_15.weights.h5')\n\n# 精度の評価をしています\nscores = model.evaluate(X_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])",
    "correct_response": "CNNを用いた画像認識【5100】 の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\nCNNを用いた画像認識【5100】 の添削課題、お疲れ様でした。\n正解です。\n\n今回の課題は転移学習の理解度を問うような課題でした。\nご提出いただきましたコードより本講座の内容をバッチリとご理解いただいていることが\n伺えます。素晴らしいです。\n\n解答例も添付いたしますので、お手隙の際にご確認ください。\n次回の添削課題もお待ちしております。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": "感情分析株価予測【6050】",
    "answer": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport MeCab\nimport re\nimport glob\nfrom datetime import datetime as dt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n'''\n記事データの形態素解析を行い、記事ごとにPN値を算出\n'''\n# MeCabインスタンスの作成、引数を無指定にするとIPA辞書になります。\nm = MeCab.Tagger('')\n\n# テキストを形態素解析し辞書のリストを返す関数\ndef get_diclist(text):\n    parsed = m.parse(text)      # 形態素解析結果（改行を含む文字列として得られる）\n    lines = parsed.split('\\n')  # 解析結果を1行（1語）ごとに分割してリスト化\n    lines = lines[0:-2]         # 後ろ2行は不要なので削除\n    diclist = []\n    for word in lines:\n        l = re.split('\\t|,',word)  # 各行はタブとカンマで区切られているため\n        d = {'BaseForm':l[7]}\n        diclist.append(d)\n    return(diclist)\n\n# 形態素解析結果の単語ごとのdictデータにPN値を追加する関数\ndef add_pnvalue(diclist_old, pn_dict):\n    diclist_new = []\n    for word in diclist_old:\n        base = word['BaseForm']        # 個々の辞書から基本形を取得\n        if base in pn_dict:\n            pn = float(pn_dict[base]) \n        else:\n            pn = 'notfound'            # その語がPN Tableになかった場合\n        word['PN'] = pn\n        diclist_new.append(word)\n    return(diclist_new)\n\n# 各記事のPN平均値を求めます。\ndef get_mean(dictlist):\n    pn_list = []\n    for word in dictlist:\n        pn = word['PN']\n        if pn!='notfound':\n            pn_list.append(pn)\n    if len(pn_list)>0:\n        pnmean = np.mean(pn_list)\n    else:\n        pnmean=0\n    return pnmean\n\n# 取得した記事の読み込み\ndef load_it():\n    created_at = []\n    texts  = []\n    files = glob.glob(\"5050_nlp_data/it-life-hack/it-life-hack-*.txt\")\n    for file in files[:400]: # 実行時間の都合上、読み込むファイルを制限しています。\n        with open(file, \"r\", encoding=\"utf-8\") as f:\n            lines = f.read().splitlines() \n            time_complex = lines[1].split(\"T\")\n            time = dt.strptime(time_complex[0], '%Y-%m-%d')\n            text = \"\".join(lines[2:])\n        texts.append(text)\n        created_at.append(time)\n    return created_at, texts \n\ncreated_at, texts = load_it()\n\n# 辞書型配列を作成して出力します。\ndicta = {'日付け': created_at, 'texts': texts} \ndf_corpus = pd.DataFrame(dicta) \n# 日付け列を時系列順にします。\ndf_corpus = df_corpus.sort_values(['日付け'])\n# 日付け列中の重複したデータを削除します。\ndf_corpus = df_corpus.drop_duplicates(['日付け'])\n# 日付け列をインデックスに設定します。\ndf_corpus = df_corpus.set_index(['日付け'])\n\n# 極性辞書を読み込み、単語と極性情報を格納します。\npn_df = pd.read_csv('./6020_negative_positive_data/data/pn_ja.dic',\\\n                    sep=':',\n                    encoding='utf-8',\n                    names=('Word','Reading','POS', 'PN')\n                   )\nword_list = list(pn_df['Word'])\npn_list   = list(pn_df['PN'])\npn_dict = dict(zip(word_list, pn_list))\n\n# 空のリストを作り、記事ごとの平均値を求めます。\nmeans_list = []\nfor text in df_corpus['texts']:\n    dl_old = get_diclist(text)\n    dl_new = add_pnvalue(dl_old, pn_dict)\n    pnmean = get_mean(dl_new)\n    means_list.append(pnmean)\ndf_corpus['pn'] = means_list\n\n'''\n株価データの終値と、日付けごとの記事データのPN値をテーブルに結合\n'''\n# chart.csvから株価情報の読み込み\ndf_chart = pd.read_csv(\"6050_stock_price_prediction_data/chart.csv\")\n\n# indexを日付けにした後、時系列に変換する。\ndf_chart[\"日付け\"] = pd.to_datetime(df_chart[\"日付け\"], format='%Y年%m月%d日')\ndf_chart[\"終値\"] = df_chart[\"終値\"].replace(',', '', regex=True)\n# カラムから'始値', '高値', '安値'を取り除いて、日付が古い順に並べる。\ndf_chart = df_chart.drop(['始値', '高値', '安値', '出来高', '前日比%'], axis=1)\n# 日付順にソート\ndf_chart = df_chart.sort_values(['日付け'])\n\n# df_corpusとdf_chartを、日付けをキーにして内部結合\ndf_table = pd.merge(df_corpus, df_chart, how=\"inner\", on = \"日付け\")\n# 日付順にソート\ndf_table = df_table.sort_values(['日付け'])\n\n'''\n訓練データ・テストデータに分割\n'''\n# XにPN値を、yに終値を格納\nX = df_table.values[:, 2]\ny = df_table.values[:, 3]\n\n# 訓練データとテストデータに分割\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\n# 訓練データとテストデータの標準化\nX_train_std = (X_train - X_train.mean()) / X_train.std()\nX_test_std = (X_test - X_train.mean()) / X_train.std()\n\n# df_trainというテーブルを作りそこにindexを日付け、カラム名をpn値、終値にして出力\ndf_train = pd.DataFrame(\n    {'pn': X_train_std,\n     '終値': y_train},\n    columns=['pn', '終値'],\n    index=df_table['日付け'][:len(X_train_std)])\n\n# テストデータについても同様にdf_testというテーブルを作成\ndf_test = pd.DataFrame(\n    {'pn': X_test_std,\n     '終値': y_test},\n    columns=['pn', '終値'],\n    index=df_table['日付け'][len(X_train_std):])\n\n'''\nPN値と株価の変化を格納\n'''\n# 日付を格納\nexchange_dates = []\n\n# 1日ごとのpn値の差分を格納する準備\npn_rates = []\npn_rates_diff = []\n\n# 1日ごとの株価の差分を格納する準備\nexchange_rates = []\nexchange_rates_diff = []\n\nprev_pn = df_train['pn'][0] # typeはfloat\nprev_exch = float(df_train['終値'][0])\n\n# 訓練データの数だけPN値・株価の変化を算出\nfor i in range(len(X_train_std)):\n    time = df_train.index[i]   # 日付け\n    pn_val = df_train['pn'][i]   # 訓練データのPN値\n    exch_val = float(df_train['終値'][i])  # 訓練データの終値\n\n    exchange_dates.append(time)  # 日付\n    pn_rates_diff.append(pn_val - prev_pn)   # PN値の変化\n    exchange_rates_diff.append(exch_val - prev_exch)   # 株価の変化\n    \n    # 前日のPN値、終値を更新\n    prev_pn = pn_val\n    prev_exch = exch_val\n\n'''\n3日間ごとのPN値と株価の変化を表示\n'''\nINPUT_LEN = 3\ndata_len = len(pn_rates_diff)\n\n# 説明変数を格納する準備\ntr_input_mat = []\n# 目的変数を格納する準備\ntr_angle_mat = []\n\n# 直近3日間なので、INPUT_LENから開始\nfor i in range(INPUT_LEN, data_len):\n    tmp_arr = []\n    # i日目の直近3日間の株価とネガポジの変化を格納\n    for j in range(INPUT_LEN):\n        tmp_arr.append(exchange_rates_diff[i-INPUT_LEN+j])\n        tmp_arr.append(pn_rates_diff[i-INPUT_LEN+j])   \n    tr_input_mat.append(tmp_arr)\n\n    # i日目の株価の上下（プラスなら1、マイナスなら0）を格納\n    if exchange_rates_diff[i] >= 0:\n        tr_angle_mat.append(1)\n    else:\n        tr_angle_mat.append(0)  \n\n# numpy配列に変換して結果を代入\ntrain_feature_arr = np.array(tr_input_mat)\ntrain_label_arr = np.array(tr_angle_mat)\n\n'''\n訓練データと同様に、テストデータに関する予測準備\n'''\n# 日付を格納\nexchange_dates_test = []\n\n# 1日ごとのpn値の差分を格納する準備\npn_rates_test = []\npn_rates_diff_test = []\n\n# 1日ごとの株価の差分を格納する準備\nexchange_rates_test = []\nexchange_rates_diff_test = []\n\n# 訓練データの数だけPN値・株価の変化を算出\nfor i in range(len(X_test_std)):\n    time = df_test.index[i]   # 日付け\n    pn_val = df_test['pn'][i]   # 訓練データのPN値\n    exch_val = float(df_test['終値'][i])  # 訓練データの終値\n\n    exchange_dates_test.append(time)  # 日付\n    pn_rates_diff_test.append(pn_val - prev_pn)   # PN値の変化\n    exchange_rates_diff_test.append(exch_val - prev_exch)   # 株価の変化\n    \n    # 前日のPN値、終値を更新\n    prev_pn = pn_val\n    prev_exch = exch_val\n\ndata_len = len(pn_rates_diff_test)\ntest_input_mat = []\ntest_angle_mat = []\n\nfor i in range(INPUT_LEN, data_len):\n    test_arr = []\n    for j in range(INPUT_LEN):\n        test_arr.append(exchange_rates_diff_test[i - INPUT_LEN + j])\n        test_arr.append(pn_rates_diff_test[i - INPUT_LEN + j])   \n    test_input_mat.append(test_arr)  # i日目の直近3日間の株価とネガポジの変化\n\n    if exchange_rates_diff[i] >= 0:  # i日目の株価の上下（プラスなら1、マイナスなら0）\n        test_angle_mat.append(1)\n    else:\n        test_angle_mat.append(0)  \n\ntest_feature_arr = np.array(test_input_mat)\ntest_label_arr = np.array(test_angle_mat)\n\n# 以下にコードを書いてください\n\n# n_neighborsの値の範囲(1から10)\nk_list = [i for i in range(1, 11)]\n\n# 正解率を格納する空リストを作成\naccuracy = []\n\n# n_neighborsを変えながらモデルを学習\nfor k in k_list:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(train_feature_arr, train_label_arr)\n    accuracy.append(model.score(test_feature_arr, test_label_arr))\n\n# グラフのプロット\nplt.plot(k_list, accuracy)\nplt.xlabel(\"n_neighbor\")\nplt.ylabel(\"accuracy\")\nplt.title(\"accuracy by changing n_neighbor\")\nplt.show()",
    "correct_response": "感情分析/株価予測【6050】の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n感情分析/株価予測【6050】の添削課題にお取り組みいただきありがとうございました。\n正解です。\n\nご提出いただきましたコードを拝見いたしますと、本講座の内容をバッチりとご理解いただいている\nように感じます。素晴らしいです。\n\nぜひこの調子で頑張ってください。\n次回のご提出もお待ちしております。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    "Notes": "##不正解時の注意事項##\n- 同、添削課題ではk近傍法で記述されていますので、k近傍法でなければ、{{修正コメント}}に下記記述を入れてください。\n--- ここから --------------------------------------------------------\n問題文とともにヒントが記載されております。\n－－－ヒント－－－－\nITライフハックの記事を感情分析して、株価の予測モデルを構築してください。\n予測モデルにはk近傍法を用います。 \nまた『教師あり学習（分類）3.3.1 パラメーター n_neighbors』を参考にしながらkの値が1~10におけるそれぞれの正答率をプロットしてください。\n－－－－－－－－－－－\nとありますので、『教師あり学習（分類）3.3.1 パラメーター n_neighbors』を参照して、回答をしていただきたく存じます。"
    },
    {
    "course": "男女識別（深層学習発展）【6100】",
    "answer": "import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Input\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras import optimizers\n\n\n# お使いの仮想環境のディレクトリ構造等によってファイルパスは異なります。\npath_male = os.listdir('./6100_gender_recognition_data/male/')\npath_female = os.listdir('./6100_gender_recognition_data/female/')\n\nimg_male = []\nimg_female = []\n\nfor i in range(len(path_male)):\n    img = cv2.imread('./6100_gender_recognition_data/male/' + path_male[i])\n    b,g,r = cv2.split(img)\n    img = cv2.merge([r,g,b])\n    img = cv2.resize(img, (50,50))\n    img_male.append(img)\n\nfor i in range(len(path_female)):\n    img = cv2.imread('./6100_gender_recognition_data/female/' + path_female[i])\n    b,g,r = cv2.split(img)\n    img = cv2.merge([r,g,b])\n    img = cv2.resize(img, (50,50))\n    img_female.append(img)\n    \nX = np.array(img_male + img_female)\ny =  np.array([0]*len(img_male) + [1]*len(img_female))\n\nrand_index = np.random.permutation(np.arange(len(X)))\nX = X[rand_index]\ny = y[rand_index]\n\n# データの分割\nX_train = X[:int(len(X)*0.8)]\ny_train = y[:int(len(y)*0.8)]\nX_test = X[int(len(X)*0.8):]\ny_test = y[int(len(y)*0.8):]\n\n# 正解ラベルをone-hotの形にします\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n# モデルにvggを使います\ninput_tensor = Input(shape=(50, 50, 3))\nvgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n\n# vggのoutputを受け取り、2クラス分類する層を定義します\n# その際中間層を下のようにいくつか入れると精度が上がります\ntop_model = Sequential()\ntop_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\ntop_model.add(Dense(256, activation='relu'))\ntop_model.add(Dropout(0.5))\ntop_model.add(Dense(2, activation='softmax'))\n\n# vggと、top_modelを連結します\nmodel = Model(vgg16.inputs, top_model(vgg16.output))\n\n# vggの層の重みを変更不能にします\nfor layer in model.layers[:19]:\n    layer.trainable = False\n\n# コンパイルします\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\n\n# 学習を行います\nmodel.fit(X_train, y_train, batch_size=100, epochs=10, validation_data=(X_test, y_test))\n\n# 画像を一枚受け取り、男性か女性かを判定する関数\ndef pred_gender(img):\n    img = cv2.resize(img, (50, 50))\n    pred = np.argmax(model.predict(np.array([img])))\n    if pred == 0:\n        return 'male'\n    else:\n        return 'female'\n\n# 精度の評価（適切なモデル名に変えて、コメントアウトを外してください）\nscores = model.evaluate(X_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\n\n# pred_gender関数に顔写真を渡して性別を予測します\nfor i in range(5):\n    img = cv2.imread('./6100_gender_recognition_data/male/' + path_male[i])\n    b,g,r = cv2.split(img) \n    img = cv2.merge([r,g,b])\n    plt.imshow(img)\n    plt.show()\n    print(pred_gender(img))",
    "correct_response": "{{コース}}の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}}の添削課題にお取り組みいただきありがとうございました。\n正解です。\n\nご提出いただきましたコードを確認いたしますと、\nVGG16モデルを用いた学習するためのコードをしっかりと記述されており\n素晴らしいです。\n\n解答例を合わせてお送りいたしますので、お手隙の際にご確認いただけますと幸いです。\n\nぜひこの調子で頑張ってください。\n次回のご提出もお待ちしております。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": [
        "理論から学ぶRNN【6521】",
        "〔2024年度シラバス対応〕理論から学ぶRNN（回帰結合型ネットワーク） ※E資格対策講座【6721】"
    ],
    "answer": "(あ):3\n(い):3",
    "correct_response": "{{コース}}の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}}の添削課題にお取り組みいただきありがとうございました。\n正解です。\n\nぜひこの調子で頑張ってください。\n次回のご提出もお待ちしております。\n'''\n回答\n{{提出コード}}\n\n解答\n(あ): 3 　(い): 3。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。\n'''\n回答\n{{提出コード}}\n\n解答\n\n(あ): 3 　(い): 3"
    },
    {
    "course": "順伝播型ネットワーク【6522】",
    "answer": "import numpy as np\n\ndef sigmoid(u):\n    return 1 / (1 + np.exp(-u))\n\ndef softmax(u):\n    return np.exp(u) / np.sum(np.exp(u))\n\ndef forward(x):\n    W1 = np.array([[0.3, 0.5], [0.4, 0.6]])\n    W2 = np.array([[0.3, 0.5], [0.4,0.6]])\n    u1 = x.dot(W1)+1\n    z1 = sigmoid(u1)\n    u2 = z1.dot(W2)+1\n    y = softmax(u2)\n    return y, z1\n\nx = np.array([[1, 0.5]])\ny, z1 = forward(x)\n\n# 誤差逆伝播法\ndef back_propagation(x, z1, y, d):\n    W1 = np.array([[0.3, 0.5], [0.4, 0.6]])\n    W2 = np.array([[0.3, 0.5], [0.4,0.6]])\n    # 順伝播型ネットワークで計算した出力と教師データとの誤差を計算してください。\n    error_range = y - d\n    grad_W2 = z1.T.dot(error_range)\n    # 活性化関数の微分\n    sigmoid_diff = z1 * (1 - z1)\n    delta = error_range.dot(W2.T) * sigmoid_diff\n    grad_W1 = x.T.dot(delta)\n\n    W2 -= learning_rate * grad_W2\n    W1 -= learning_rate * grad_W1\n    return W1,W2\n    \n# 教師データ\nd = np.array([[1, 0]])\n# 学習データ\nlearning_rate = 0.005\nW1,W2 = back_propagation(x, z1, y, d)\n\nprint(W1)\nprint()\nprint(W2)",
    "correct_response": "{{コース}}の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}}の添削課題にお取り組みいただきありがとうございました。\n正解です。\n\n今回は、順伝播型ネットワークで計算した出力と教師データとの誤差を計算してerror_rangeに代入する問題でした。\n誤差逆伝播法はディープラーニングにとって非常に重要なアルゴリズムとなります。\n\n解答例を合わせてお送りいたしますので、お手隙の際にご確認いただけますと幸いです。\n\nぜひこの調子で頑張ってください。\n次回のご提出もお待ちしております。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": [
        "深層モデルのための最適化【6524】",
        "〔2024年度シラバス対応〕深層モデルのための最適化 ※E資格対策講座【6906】"
    ],
    "answer": "import numpy as np\n\n# モメンタム法の実装\nclass Momentum:\n    # 初期化\n    def __init__(self, lr=0.01, momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.v = None\n\n    # 更新\n    def update(self, params, grads):\n        # vになにもない時、配列を0で初期化\n        if self.v is None:\n            self.v = {}\n            for key, value in params.items():\n                self.v[key] = np.zeros_like(value)\n\n        # 数式の実装        \n        for key in params.keys():\n            # vを計算してください\n            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n            \n            # paramsを更新してください\n            params[key] += self.v[key]",
    "correct_response": "{{コース}}の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}}の添削課題にお取り組みいただきありがとうございました。\n正解です。\n\n今回の添削課題はモメンタム法を実装していただきました。\nコードを拝見しましたところ、バッチリ実装できておりました。\n素晴らしいです。\n\nぜひこの調子で頑張ってください。\n次回のご提出もお待ちしております。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": "深層学習のための正則化【6525】",
    "answer": "# ドロップアウト実装\n\nimport numpy as np\nfrom keras.datasets import mnist\n\n# mnistのデータセットをロード\n\n(x_train, t_train), (x_test, t_test) = mnist.load_data()\n\n# データを削減\nx_train = x_train[:300]\nt_train = t_train[:300]\n\n# ドロップアウト\nclass Dropout:\n        def __init__(self, dropout_ratio=0.5):\n            self.dropout_ratio = dropout_ratio\n            self.mask = None\n\n        def forward(self, x, train_flg=True):\n            if train_flg:\n                self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n                return x * self.mask\n            else:\n                return x * (1.0 - self.dropout_ratio)\n\n        def backward(self, dout):\n            return dout * self.mask\n        \ndrop = Dropout()\ny = drop.forward(t_train)\n\nprint('ドロップアウト前')\nprint(t_train)\nprint()\nprint('ドロップアウト後')\nprint(y)",
    "correct_response": "{{コース}}の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}}の添削課題にお取り組みいただきありがとうございました。\n正解です。\n\n今回の添削課題はドロップアウトを適用する前後でどのような処理が行われているのか\nを確認することがゴールとなっております。\n\nご提出いただきましたコードを拝見いたしますとバッチリとご理解いただいているように\n感じます。素晴らしいです\n\nぜひこの調子で頑張ってください。\n次回のご提出もお待ちしております。",
    "incorrect_response": "{{コース}} の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}} の添削課題のご提出お疲れさまでした。\n 残念ながら不正解です。\n\n{{修正コメント}}\n\n解答例を添付いたしますので、内容のご確認をお願いいたします。\n\n再提出の必要はございませんので、先に、お進みください。\n次回の提出もお待ちしております。"
    },
    {
    "course": "理論から学ぶ強化学習【6526】",
    "answer": "Q(0,0)=Q(0,0)+0.5×( 0+0.8×max(Q(1,1),Q(1,0))-Q(0,0))\n      =0.0+0.5×( 0+0.8×5.0-0.0)\n　　　=2.0\nQ(1,0)=Q(1,0)+0.5×( 0+0.8×max(Q(0,0),Q(0,1))-Q(1,0))\n      =0.0\nQ(0,1)=Q(0,1)+0.5×( 0+0.8×max(Q(0,0),Q(0,1))-Q(0,1))\n      =0.0\nQ(1,1)=Q(1,1)+0.5×(10+0.8×max(Q(1,0),Q(1,1))-Q(1,1))\n      =5.0+0.5×(10+0.8×5.0-5.0)=5.0+4.5\n　　　=9.5",
    "correct_response": "{{コース}}の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}}の添削課題にお取り組みいただきありがとうございました。\n正解です。\n\n引き続きご学習を頑張っていきましょう。\n次回のご提出もお待ちしております。\n\n【ご回答】\n{{提出コード}}\n\n【解答】\nQ(0,0)=Q(0,0)\n+0.5 × (0+0.8×max(Q(1,1),Q(1,0))-Q(0,0))\n=0.5 × (0 + 0.8×5 - 0.0)=2.0\n\nQ(1,0)=Q(1,0)\n+ 0.5 × (0 + 0.8×max(Q(0,0),Q(0,1)) - Q(1,0))=0.0\n\nQ(0,1)=Q(0,1)+0.5×(0+0.8×Q(0,0)-Q(0,1))=0.0\n\nQ(1,1)=Q(1,1)\n+0.5x(10+0.8 × max(Q(1,0),Q(1,1))-Q(1,1))\n=5.0+0.5×(10+0.8×5.0-5.0)=5.0+4.5=9.5",
    "incorrect_response": "{{コース}}の添削問題のご提出ありがとうございます。\n添削結果を返却致します。\n------\n\n{{コース}}の添削課題にお取り組みいただきありがとうございました。\n残念ながら不正解です。\n\n解答例は以下となります。\nお手隙の際にご確認いただけますと幸いです。\n\n再提出は不要です。\n\n【ご回答】\n{{提出コード}}\n\n【解答】\nQ(0,0)=Q(0,0)\n+0.5 × (0+0.8×max(Q(1,1),Q(1,0))-Q(0,0))\n=0.5 × (0 + 0.8×5 - 0.0)=2.0\n\nQ(1,0)=Q(1,0)\n+ 0.5 × (0 + 0.8×max(Q(0,0),Q(0,1)) - Q(1,0))=0.0\n\nQ(0,1)=Q(0,1)+0.5×(0+0.8×Q(0,0)-Q(0,1))=0.0\n\nQ(1,1)=Q(1,1)\n+0.5x(10+0.8 × max(Q(1,0),Q(1,1))-Q(1,1))\n=5.0+0.5×(10+0.8×5.0-5.0)=5.0+4.5=9.5"
    }
]
